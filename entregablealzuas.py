# -*- coding: utf-8 -*-
"""EntregableAlzuas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13aIC6rnC7kr5Vzk7--F1ahAG3cHeDU5I

# Estudio completo de un problema de ML con Python
### Estimación del precio de una vivienda

En este notebook haremos un análisis exploratorio básico de la base de datos de viviendas de airbnb.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy  as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

"""## 1. Carga de datos y división train/test

### Cargamos Datos
"""

house_data_unprocessed = pd.read_csv("./data/airbnb-listings-extract.csv",delimiter = ';') # cargamos fichero
print(house_data_unprocessed.shape)

pd.options.display.max_columns = None #Vemos todas las columnas

#Quitamos la columna de urls ya que no nos va a aportar ninguna información
house_data=house_data_unprocessed.drop(columns=['Listing Url','Thumbnail Url','Medium Url','Picture Url','XL Picture Url',
'Host URL','Host Thumbnail Url','Host Picture Url'])

print(house_data.shape)
house_data.head(5)

"""### Dividimos y guardámos datos"""

from sklearn.model_selection import train_test_split

full_df = house_data
train, test = train_test_split(full_df, test_size=0.2, shuffle=True, random_state=0)

print(f'Dimensiones del dataset de training: {train.shape}')
print(f'Dimensiones del dataset de test: {test.shape}')

# Guardamos
train.to_csv('./data/airbnb-listings-extract_train.csv', sep=';', decimal='.', index=False)
test.to_csv('./data/airbnb-listings-extract_test.csv', sep=';', decimal='.', index=False)

# A partir de este momento cargamos el dataset de train y trabajamos ÚNICAMENTE con él.

house_data = pd.read_csv('./data/airbnb-listings-extract_train.csv', sep=';', decimal='.')
house_data.head(5).T

"""## 2. Análisis exploratorio

Podemos analizar la estructura básica del dataset con las funciones de Pandas que ya conocemos: `describe`, `dtypes`, `shape`, etc.
"""

house_data.describe()

house_data.info()

# Mostramos estadísticas descriptivas
# Separamos las columnas numéricas y categóricas para luego decir como rellenaremos sus nulos
numerical_cols = house_data.select_dtypes(include=['number']).columns.tolist()
categorical_cols = house_data.select_dtypes(include=['object', 'category']).columns.tolist()

print(categorical_cols)

numerical_more_than_10 = [] # Dividimos las categóricas en las que tienen más de 10 valores distintos
numerical_less_equal_10 = [] # Y los que tienen menos de 10 valores distintos

for col in numerical_cols:
    unique_values = house_data[col].nunique()
    if unique_values > 10:
        numerical_more_than_10.append(col)
    else:
        numerical_less_equal_10.append(col)

# Función para detectar outliers utilizando el método IQR
def detect_outliers(df, cols):
    columns_with_outliers = []
    columns_without_outliers = []

    for col in cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

        if not outliers.empty:
            columns_with_outliers.append(col)
        else:
            columns_without_outliers.append(col)

    return columns_with_outliers, columns_without_outliers

# Detectamos si las columnas numéricas con más de 10 valores distintos que tienen outliers
columns_with_outliers, columns_without_outliers = detect_outliers(house_data, numerical_more_than_10)

print("Columnas con outliers:", columns_with_outliers)
print("Columnas sin outliers:", columns_without_outliers)

# Mirámos también que columnas contienen valores nulos
null_check = house_data.isnull().any()
columns_with_nulls = null_check[null_check].index.tolist()

print(columns_with_nulls)

# Función para encontrar los elementos comunes entre dos listas
def find_common_elements(list1, list2):
    return [col for col in list1 if col in list2]

# Ahora para poder decidir como rellenamos clasificamos las columnas en función de si tienen nulos y/o outliers

# Columnas numéricas con más de 10 valores distintos
columns_with_outliers_nulls = find_common_elements(columns_with_outliers, columns_with_nulls)
columns_without_outliers_nulls = find_common_elements(columns_without_outliers, columns_with_nulls)

numerical_less_equal_10_nulls = find_common_elements(numerical_less_equal_10, columns_with_nulls)
categorical_cols_nulls = find_common_elements(categorical_cols, columns_with_nulls)

# Impresión de las nuevas listas
print("columns_with_outliers_nulls:", columns_with_outliers_nulls)
print("columns_without_outliers_nulls:", columns_without_outliers_nulls)
print("numerical_less_equal_10_nulls:", numerical_less_equal_10_nulls)
print("categorical_cols_nulls:", categorical_cols_nulls)

"""#### Imputación

Ya tenemos toda la información preparada para imputar los valores nulos, para ello lo que haremos es lo siguiente:
1. Columnas numéricas con outliers y con nulos y teniendo más de 10 valores distintos: los rellenaremos con la mediana ya que así no nos perjudican estos outliers.
2. Columnas numéricas sin outliers y con nulos y teniendo más de 10 valores distintos: los rellenaremos con la media, ya que al no tener outliers la media es más efectiva. En este caso no tenemos.
3. Para las columnas categóricas: rellenaremos los nulos con 'Desconocido' y después las codificaremos.
4. Columnas numéricas con nulos y teniendo menos de 10 valores distintos: los rellenaremos con la moda.
"""

# Iteramos sobre cada columna en la lista y rellenamos los nulos con la mediana
for column in columns_with_outliers_nulls:
    house_data[column].fillna(house_data[column].median(), inplace=True)

# Iteramos sobre cada columna en la lista y rellenamos los nulos con la moda
for column in numerical_less_equal_10_nulls:
    house_data[column].fillna(house_data[column].mode()[0], inplace=True)
pd.set_option('display.max_rows', None)

house_data.isnull().any()

"""Nos queda rellenar los valores nulos de las variables categóricas.

#### Codificación de variables categóricas

Pasamos a realizar la codificación de las variables categóricas.
"""

!pip install category_encoders

from category_encoders import TargetEncoder
import pandas as pd

# Sustituimos los nulos por 'Desconocido'
house_data[categorical_cols] = house_data[categorical_cols].fillna('Desconocido')

# Especificamos la variable objetivo
variable_objetivo = 'Price'

# Aplicamos TargetEncoder
encoder = TargetEncoder(cols=categorical_cols)
house_data_encoded = encoder.fit_transform(house_data, house_data[variable_objetivo])

# Guardamos los datos codificados
house_data_encoded.to_csv('./data/archivo_codificado.csv', index=False)

"""Revisamos que efectivamente nos ha quedado un dataset sin nulos y todo con números."""

house_data_encoded.isnull().any()

house_data_encoded.head(50)

# Eliminamos las columnas id y las que tienen que ver con la fecha del scrapeo,
# ya que no nos van a dar ninguna información
listaDROP = [
    'ID', 'Scrape ID', 'Host ID', 'Last Scraped', 'Calendar Updated', 'Calendar last Scraped']

house_data_encoded = house_data_encoded.drop(listaDROP, axis=1)

"""## Correlación
Estudiamos ahora la correlación que pueda haber entre distintas columnas y que no hayamos tenido en cuenta.
"""

import seaborn as sns
corr = np.abs(house_data_encoded.drop(['Price'], axis=1).corr())

mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(14, 12))

sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,
            linewidths=.3, cmap="YlGnBu", cbar_kws={"shrink": .8})

plt.show()

"""Viendo la correlación que hay entre las columnas decidimos eliminar Calculated host listing count ya que
depende de Host Total Listings Count. Eliminamos también Has Availability ya que si tiene Availability 365
tiene también Availability. Además como dejamos el Zip code y la longitud y latitud podemos quitar
Smart Location y Country Code ya que se vuelven innecesarios, además quitamos Notes y Access ya que vienen
incluidas probablemente en las notas de la casa.
"""

listaDROP2 = [
    'Calculated host listings count', 'Has Availability', 'Smart Location', 'Notes', 'Access', 'Country Code']

house_data_encoded = house_data_encoded.drop(listaDROP2, axis=1)

"""Pintamos la correlación otra vez a ver si ha cambiado algo."""

import seaborn as sns
corr = np.abs(house_data_encoded.drop(['Price'], axis=1).corr())

mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(14, 12))

sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,
            linewidths=.3, cmap="YlGnBu", cbar_kws={"shrink": .8})

plt.show()

"""Nos quedamos con lo que tenemos ya que no vemos más correlaciones fuertes.

## 3. Visualización (y más análisis)
"""

# Mostramos estadísticas descriptivas
# Separamos las columnas numéricas para luego decir como rellenaremos sus nulos
numerical_cols = house_data.select_dtypes(include=['number']).columns.tolist()

# Detectamos outliers en las columnas numéricas con más de 10 valores únicos
columns_with_outliers, columns_without_outliers = detect_outliers(house_data, numerical_cols)

print("Columnas con outliers:", columns_with_outliers)

numerical_columns2 = [col for col in columns_with_outliers if col not in categorical_cols]

listaDROP3 = listaDROP + listaDROP2
# Filtramos la lista categorical_columns2 eliminando los elementos de listaDROP3
numerical_columns2_filtradas = [col for col in numerical_columns2 if col not in listaDROP3]

print(numerical_columns2_filtradas)

"""Visualizamos las variables numéricas para poder realizar un análisis sobre sus outliers y si tienen sentido."""

for k in numerical_columns2_filtradas:
    plt.figure(figsize=(15, 6))

    # Scatter plot con Price
    plt.subplot(1, 3, 1)
    sns.scatterplot(x=house_data_encoded[k], y=house_data_encoded['Price'])
    plt.title(k + ' vs Price')
    plt.xlabel(k)
    plt.ylabel('Price')

    # Histograma
    plt.subplot(1, 3, 2)
    sns.histplot(house_data_encoded[k], bins=30, kde=True)
    plt.title('Histograma de ' + k)
    plt.xlabel(k)
    plt.ylabel('Frecuencia')

    # Boxplot
    plt.subplot(1, 3, 3)
    sns.boxplot(y=house_data_encoded[k])
    plt.title('Boxplot de ' + k)

    plt.tight_layout()
    plt.show()

"""Revisamos los datos de las siguientes columnas para ayudarnos a pensar las condiciones de filtrado si fueran necesarias."""

for k in ['Bedrooms', 'Square Feet', 'Extra People', 'Minimum Nights', 'Maximum Nights', 'Host Acceptance Rate']:
    print(house_data_encoded[k].value_counts())

# Filtramos el dataframe sin los outliers haciendo uno nuevo.

conditions = (
    (house_data_encoded['Minimum Nights'] <= 60 ) &
    (house_data_encoded['Host Acceptance Rate'] <= 100) &
    (house_data_encoded['Square Feet'] > 0) )

house_data_encoded_sin_outliers = house_data_encoded[conditions]

print(f"Tamaño del dataframe original: {house_data_encoded.shape}")
print(f"Tamaño del dataframe sin outliers: {house_data_encoded_sin_outliers.shape}")

"""Así nos queda el data frame una vez aplicacadas las normas elegidas en el código anterior."""

house_data_encoded_sin_outliers.describe()

"""## 4. Generación de nuevas características

Generamos la variable metros cuadrados junto con la variable precio por metros cuadrados a ver si nos resultan útiles en el modelo.
"""

house_data_encoded_sin_outliers['m2'] = house_data_encoded_sin_outliers['Square Feet'] * 0.3048 * 0.3048

#house_data_encoded_sin_outliers['m2'] = house_data_encoded_sin_outliers['Square Feet'].apply(sqft_to_m2)

house_data_encoded_sin_outliers['Price_Agains_m2'] = house_data_encoded_sin_outliers['Price']/house_data_encoded_sin_outliers['m2']
house_data_encoded_sin_outliers.info()

"""## 5. Modelado, cross-validation y estudio de resultados en train y test

Combinamos todo nuestro preprocesamiento en una única celda:
"""

from category_encoders import TargetEncoder
import pandas as pd
from sklearn.preprocessing import LabelEncoder


# Carga de datos
house_data = pd.read_csv('./data/airbnb-listings-extract_train.csv', sep=';', decimal='.')

# Imputación
# Iteramos sobre cada columna en la lista y rellenamos los nulos con la mediana
for column in ['Host Response Rate', 'Host Listings Count', 'Host Total Listings Count', 'Bathrooms', 'Beds', 'Square Feet', 'Price', 'Weekly Price', 'Monthly Price', 'Security Deposit', 'Cleaning Fee', 'Review Scores Rating', 'Calculated host listings count', 'Reviews per Month']:
    house_data[column].fillna(house_data[column].median(), inplace=True)

# Iteramos sobre cada columna en la lista y rellenamos los nulos con la moda
for column in ['Bedrooms', 'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication', 'Review Scores Location', 'Review Scores Value']:
    house_data[column].fillna(house_data[column].mode()[0], inplace=True)

# Imputación variables catergóricas
categorical_cols_nulls = ['Summary', 'Space', 'Neighborhood Overview', 'Notes', 'Transit', 'Access', 'Interaction', 'House Rules', 'Host Location', 'Host About', 'Host Response Time', 'Host Acceptance Rate', 'Host Neighbourhood', 'Host Verifications', 'Neighbourhood', 'Neighbourhood Group Cleansed', 'City', 'State', 'Zipcode', 'Market', 'Amenities', 'Has Availability', 'First Review', 'Last Review', 'License', 'Jurisdiction Names', 'Last Scraped', 'Name', 'Description', 'Experiences Offered', 'Host Name', 'Host Since', 'Street', 'Neighbourhood Cleansed', 'Smart Location', 'Country Code', 'Country', 'Property Type', 'Room Type', 'Bed Type', 'Calendar Updated', 'Calendar last Scraped', 'Cancellation Policy', 'Geolocation', 'Features', 'Host Response Rate', 'Bathrooms', 'Bedrooms', 'Beds', 'Square Feet', 'Price', 'Weekly Price', 'Monthly Price', 'Security Deposit', 'Cleaning Fee', 'Review Scores Rating', 'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication', 'Review Scores Location', 'Review Scores Value', 'Reviews per Month']

# Sustituimos nulos por 'Desconocido'
house_data[categorical_cols_nulls] = house_data[categorical_cols_nulls].fillna('Desconocido')

# Especificamos la variable objetivo
variable_objetivo = 'Price'

# Aplicamos TargetEncoder
encoder = TargetEncoder(cols=categorical_cols_nulls)
house_data_encoded = encoder.fit_transform(house_data, house_data[variable_objetivo])

# Guardamos los datos codificados
house_data_encoded.to_csv('./data/archivo_codificado_train.csv', index=False)

# Eliminamos las columnas sin interés
listaDROP2 = ['Calculated host listings count', 'Has Availability', 'Smart Location', 'Notes', 'Access', 'Country Code']
listaDROP = ['ID', 'Scrape ID', 'Host ID', 'Last Scraped', 'Calendar Updated', 'Calendar last Scraped']
listaDROP3 = listaDROP + listaDROP2

house_data_encoded=house_data_encoded.drop(columns=listaDROP3)

# Filtramos el dataframe sin los outliers haciendo uno nuevo.

conditions = (
    (house_data_encoded['Minimum Nights'] <= 60 ) &
    (house_data_encoded['Host Acceptance Rate'] <= 100) &
    (house_data_encoded['Square Feet'] > 0) )

house_data_encoded_sin_outliers = house_data_encoded[conditions]


# Generamos características
# Añadimos variable metro cuadrado
house_data_encoded_sin_outliers['m2'] = house_data_encoded_sin_outliers['Square Feet'] * 0.3048 * 0.3048
# Añadimos variable de precio por metro cuadrado
house_data_encoded_sin_outliers['Price_Agains_m2'] = house_data_encoded_sin_outliers['Price']/house_data_encoded_sin_outliers['m2']
house_data_encoded_sin_outliers.info

"""Y ahora aplicamos fácilmente a test:"""

# Carga de datos
house_data_test = pd.read_csv('./data/airbnb-listings-extract_test.csv', sep=';', decimal='.')

# Imputación
# Iteramos sobre cada columna en la lista y rellenamos los nulos con la mediana
for column in ['Host Response Rate', 'Host Listings Count', 'Host Total Listings Count', 'Bathrooms', 'Beds', 'Square Feet', 'Price', 'Weekly Price', 'Monthly Price', 'Security Deposit', 'Cleaning Fee', 'Review Scores Rating', 'Calculated host listings count', 'Reviews per Month']:
    house_data_test[column].fillna(house_data[column].median(), inplace=True)

# Iteramos sobre cada columna en la lista y rellenamos los nulos con la moda
for column in ['Bedrooms', 'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication', 'Review Scores Location', 'Review Scores Value']:
    house_data_test[column].fillna(house_data[column].mode()[0], inplace=True)

# Imputación variables catergóricas
categorical_cols_nulls = ['Summary', 'Space', 'Neighborhood Overview', 'Notes', 'Transit', 'Access', 'Interaction', 'House Rules', 'Host Location', 'Host About', 'Host Response Time', 'Host Acceptance Rate', 'Host Neighbourhood', 'Host Verifications', 'Neighbourhood', 'Neighbourhood Group Cleansed', 'City', 'State', 'Zipcode', 'Market', 'Amenities', 'Has Availability', 'First Review', 'Last Review', 'License', 'Jurisdiction Names']
# Sustituir nulos por 'Desconocido'
house_data_test[categorical_cols_nulls] = house_data[categorical_cols_nulls].fillna('Desconocido')

# Aplicamos el encoder al conjunto de datos de prueba
house_data_test_encoded = encoder.transform(house_data_test)

# Guardamos los datos codificados
house_data_test_encoded.to_csv('./data/archivo_codificado_test.csv', index=False)

# Eliminamos las columnas sin interés
listaDROP2 = ['Calculated host listings count', 'Has Availability', 'Smart Location', 'Notes', 'Access', 'Country Code']
listaDROP = ['ID', 'Scrape ID', 'Host ID', 'Last Scraped', 'Calendar Updated', 'Calendar last Scraped']
listaDROP3 = listaDROP + listaDROP2

house_data_test_encoded=house_data_test_encoded.drop(columns=listaDROP3)

# Filtramos el dataframe sin los outliers haciendo uno nuevo.

conditions = (
    (house_data_test_encoded['Minimum Nights'] <= 60 ) &
    (house_data_test_encoded['Host Acceptance Rate'] <= 100) &
    (house_data_test_encoded['Square Feet'] > 0) )

house_data_test_encoded_sin_outliers = house_data_test_encoded[conditions]


# Generamos características
# Añadimos variable metro cuadrado
house_data_test_encoded_sin_outliers['m2'] = house_data_test_encoded_sin_outliers['Square Feet'] * 0.3048 * 0.3048
# Añadimos variable de precio por metro cuadrado
house_data_test_encoded_sin_outliers['Price_Agains_m2'] = house_data_test_encoded_sin_outliers['Price']/house_data_test_encoded_sin_outliers['m2']
pd.set_option('display.max_rows', None)

"""Ahora podemos preparar los datos para sklearn:"""

from sklearn import preprocessing

# Dataset de train
data_train = house_data_encoded_sin_outliers.values
y_train = data_train[:,40:41]     # nos quedamos con la columna, price
X_train = np.delete(data_train, 40, axis=1)     # nos quedamos con el resto

# Dataset de test
data_test = house_data_test_encoded_sin_outliers.values
y_test = data_test[:,40:41]   # nos quedamos con la columna, price
X_test = np.delete(data_test, 40, axis=1)     # nos quedamos con el resto

"""Y si queremos, podemos normalizar, pero con los datos de train!"""

# Escalamos (con los datos de train)
scaler = preprocessing.StandardScaler().fit(X_train)
XtrainScaled = scaler.transform(X_train)

# recordad que esta normalización/escalado la realizo con el scaler anterior, basado en los datos de training!
XtestScaled = scaler.transform(X_test)

print('Datos entrenamiento: ', XtrainScaled.shape)
print('Datos test: ', XtestScaled.shape)

"""Buscámos el alpha óptimo."""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Lasso

alpha_vector = np.logspace(-4,4,50)
param_grid = {'alpha': alpha_vector }
grid = GridSearchCV(Lasso(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 5, verbose=2)
grid.fit(XtrainScaled, y_train)
print("best mean cross-validation score: {:.5f}".format(grid.best_score_))
print("best parameters: {}".format(grid.best_params_))

#-1 porque es negado
scores = -1*np.array(grid.cv_results_['mean_test_score'])
plt.semilogx(alpha_vector,scores,'-o')
plt.xlabel('alpha',fontsize=16)
plt.ylabel('5-Fold MSE')
plt.show()

from sklearn.metrics import mean_squared_error

alpha_optimo = grid.best_params_['alpha']
lasso = Lasso(alpha = alpha_optimo).fit(XtrainScaled,y_train)

ytrainLasso = lasso.predict(XtrainScaled)
ytestLasso  = lasso.predict(XtestScaled)
mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)
mseTestModelLasso = mean_squared_error(y_test,ytestLasso)

print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)
print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)

print('RMSE Modelo Lasso (train): %0.3g' % np.sqrt(mseTrainModelLasso))
print('RMSE Modelo Lasso (test) : %0.3g' % np.sqrt(mseTestModelLasso))

feature_names = house_data.columns[1:] # es igual en train y en test

w = lasso.coef_
for f,wi in zip(feature_names,w):
    print(f,wi)

"""### Análisis de Resultados del Modelo Lasso

En nuestra reciente iteración del modelo Lasso, hemos observado mejoras significativas en las métricas de error:

- **MSE (train):** 0.468
- **MSE (test):** 0.339
- **RMSE (train):** 0.684
- **RMSE (test):** 0.583

Estos resultados indican que nuestro modelo se ajusta bien a los datos de entrenamiento y generaliza adecuadamente en el conjunto de prueba.

### Coeficientes del Modelo

Características relevantes:
- **Room Type:** 1.5896
- **Review Scores Checkin:** 37.3923
- **Review Scores Cleanliness:** 0.1056
- **House Rules:** 0.0801

### Interpretación de Resultados

1. **Exactitud y Generalización:** Las bajas diferencias entre las métricas de entrenamiento y prueba muestran un buen equilibrio, evitando el sobreajuste.
2. **Importancia de Características:** `Room Type` y `Review Scores Checkin` son factores clave en las predicciones, destacando su relevancia.
3. **Características Irrelevantes:** Muchas características tienen coeficientes cercanos a cero y pueden ser excluidas en futuras iteraciones para simplificar el modelo.

En resumen, nuestro modelo Lasso ha mostrado un rendimiento robusto con mejoras significativas en las métricas de error, identificando características clave que pueden guiar futuras mejoras.

## Random Forest
"""

from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Convertir y_train y y_test a arrays unidimensionales si son DataFrames
y_train = np.ravel(y_train)
y_test = np.ravel(y_test)

rf = RandomForestRegressor()
param_grid = {
    'n_estimators': [10, 100],
    'max_features': ['auto', 'sqrt'],
    'max_depth': [10, 20],
}
rf_cv = GridSearchCV(rf, param_grid, cv=10, scoring='neg_mean_squared_error',verbose=2)
rf_cv.fit(XtrainScaled, y_train)

# Mejor modelo y métricas
best_rf = rf_cv.best_estimator_
y_train_pred = best_rf.predict(XtrainScaled)
y_test_pred = best_rf.predict(XtestScaled)

mse_train_rf = mean_squared_error(y_train, y_train_pred)
mse_test_rf = mean_squared_error(y_test, y_test_pred)
rmse_train_rf = mse_train_rf**0.5
rmse_test_rf = mse_test_rf**0.5

print(f'MSE Random Forest (train): {mse_train_rf}')
print(f'MSE Random Forest (test) : {mse_test_rf}')
print(f'RMSE Random Forest (train): {rmse_train_rf}')
print(f'RMSE Random Forest (test) : {rmse_test_rf}')

"""Estas métricas indican que el modelo Random Forest ha logrado ajustar bastante bien los datos de entrenamiento, con un error relativamente bajo. Las métricas de error en el conjunto de prueba son significativamente más altas que en el conjunto de entrenamiento. Esto sugiere que el modelo podría estar sobreajustado (overfitting) a los datos de entrenamiento. El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento y no generaliza bien en datos no vistos. Además nos sale un warning, vamos a intentar solucionar estos problemas ajustando los parámetros."""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Asegúrate de que y_train y y_test sean arrays unidimensionales
y_train = np.ravel(y_train)
y_test = np.ravel(y_test)

# Verificar valores NaN e infinitos en los datos
print("Valores NaN en X_train:", np.isnan(XtrainScaled).sum())
print("Valores NaN en y_train:", np.isnan(y_train).sum())
print("Valores infinitos en X_train:", np.isinf(XtrainScaled).sum())
print("Valores infinitos en y_train:", np.isinf(y_train).sum())

# Definir el modelo y el grid de hiperparámetros
rf = RandomForestRegressor()
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [5, 10],
    'min_samples_split': [10, 20],
    'min_samples_leaf': [4, 6]
}

# Configurar Grid Search con validación cruzada
rf_cv = GridSearchCV(rf, param_grid, cv=10, scoring='neg_mean_squared_error', return_train_score=True)
rf_cv.fit(X_train, y_train)

# Verificar si GridSearchCV se ajustó correctamente
print(f'Mejores parámetros encontrados: {rf_cv.best_params_}')
print(f'Mejor puntuación (neg_mean_squared_error): {rf_cv.best_score_}')

# Obtener el mejor modelo
best_rf = rf_cv.best_estimator_

# Predicciones
y_train_pred = best_rf.predict(X_train)
y_test_pred = best_rf.predict(X_test)

# Calcular y mostrar las métricas de error
mse_train_rf = mean_squared_error(y_train, y_train_pred)
mse_test_rf = mean_squared_error(y_test, y_test_pred)
rmse_train_rf = mse_train_rf**0.5
rmse_test_rf = mse_test_rf**0.5

print(f'MSE Random Forest (train): {mse_train_rf}')
print(f'MSE Random Forest (test) : {mse_test_rf}')
print(f'RMSE Random Forest (train): {rmse_train_rf}')
print(f'RMSE Random Forest (test) : {rmse_test_rf}')

"""## Conclusiones

### Error en train

El modelo de Lasso presenta un MSE y RMSE mucho más bajos en el conjunto de entrenamiento en comparación con el modelo de Random Forest, lo que indica que Lasso ha ajustado los datos de entrenamiento de manera más precisa.
Random Forest muestra un error considerablemente mayor en el conjunto de entrenamiento. Esto sugiere que el modelo ha ajustado los datos de entrenamiento con menos precisión en términos absolutos, pero hay que considerar que Random Forest tiende a tener una mayor varianza debido a su naturaleza más compleja.

### Error en Prueba

Para el conjunto de prueba, Lasso también presenta errores (MSE y RMSE) mucho más bajos en comparación con Random Forest. Esto indica que Lasso está generalizando mejor a datos no vistos.
Random Forest tiene un MSE y RMSE extremadamente altos en el conjunto de prueba, lo que indica un problema significativo de sobreajuste. A pesar de su mejor rendimiento en el entrenamiento (en términos de RMSE), falla en generalizar correctamente, lo que sugiere que el modelo está sobreajustado a los datos de entrenamiento y no funciona bien con nuevos datos.

### En resumen

Lasso Regression parece ser el modelo superior en este caso, ya que presenta tanto un error de entrenamiento como de prueba más bajos. Esto sugiere que Lasso es capaz de capturar las relaciones subyacentes en los datos sin sobreajustarse a ellos, lo que resulta en un mejor rendimiento en datos no vistos.
Random Forest, aunque más flexible y poderoso en teoría, muestra un severo sobreajuste en este escenario. Esto se manifiesta en su excelente rendimiento en los datos de entrenamiento pero un rendimiento muy pobre en los datos de prueba. Este comportamiento sugiere que el modelo está capturando demasiado ruido en los datos de entrenamiento y no logra generalizar.
"""